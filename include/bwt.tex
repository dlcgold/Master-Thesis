\section{Trasformata di Burrows-Wheeler}
\label{secbwt}
Introdotta nel 1994 da Burrows e Wheeler con lo scopo di comprimere testi, la
\textbf{Burrows-Wheeler Transform} \cite{bwt} è divenuta ormai uno standard nel
campo dell'\textit{algoritmica su stringhe} e della \textit{bioinformatica},
grazie ai suoi molteplici vantaggi sia dal punto di vista della complessità
temporale che da quello della complessità spaziale.\\
Nel dettaglio la \textit{BWT} è una \textit{trasformata reversibile} che
permette una \textit{compressione lossless}, quindi senza perdita
d'informazione. Tale trasformazione vien costruita a partire dal riordinamento
dei caratteri del testo in input, fattore che ha 
portato all'evidenza per cui caratteri uguali tendono ad essere posti
consecutivamente all'interno della stringa prodotta dalla trasformata.
\begin{definizione}
  Dato un testo $T$ \$-terminato, tale che $|T|=n$, si definisce la
  \textbf{Burrows-Wheeler Transform (\textit{BWT})} di $T$, denotata con
  $BWT_T$, come un array di caratteri lungo $n$ dove l'elemento $i$-esimo è il
  carattere che precede l'$i$-esimo suffisso $T$ nel riordinamento
  lessicografico. Più formalmente si ha che, con $0\leq i<n$:
  \[BWT_T[i]=
    \begin{cases}
      T[SA_T[i]-1]&\mbox{ se } SA_T[i]\neq 1\\
      \$&\mbox{ altrimenti}
    \end{cases}
  \]
\end{definizione}
In termini più pratici, la \textit{BWT} di un testo è calcolabile riordinando
lessicograficamente tutte le possibili \textbf{rotazioni} del testo $T$.
\begin{definizione}
  Si definisce \textbf{rotazione $\mathbf{i}$-esima}, denotata con $rot_T(i)$ di
  un testo $T$, tale che $|T|=n$, come la stringa ottenuta dalla concatenazione
  del suffisso $i$-esimo con la restante porzione del testo. Più formalmente si
  ha che, avendo $0\leq i<n$:
  \[rot_T(i)=T[i,n-1]\cdot T[0,i-1]\]
\end{definizione}
Data questa definizione quindi la \textit{BWT} del testo $T$ risulta essere
l'ultima colonna della matrice che si ottiene riordinando tutte le
\textit{rotazioni} di $T$, che altro non sono che i suffissi già riordinati per
il calcolo del \textit{SA} a cui viene concatenata la parte restante del
testo.\\
Un altro array spesso utilizzato insieme alla \textit{BWT} è il cosiddetto
\textbf{array $\mathbf{F}$}, lungo $|T|$, che altro non è che l'array formato
dalla prima colonna della matrice delle rotazioni. In termini ancora più
semplicistici l'array $F$ è banalmente l'array formato dal riordinamento
lessicografico dei caratteri del testo $T$.\\
Per chiarezza si vede un esempio. 
\begin{esempio}
   Si prenda la stringa:
  \[s=\mbox{mississippi\$},\,\,|s|=12\]
  Si produce la seguente matrice delle rotazioni riordinate:
  \begin{table}[H]
    \centering
    \footnotesize
    \begin{tabular}{c|c|c|c|c} 
      \textbf{Indice} & $\mathbf{SA_T}$ & $\mathbf{F_T}$ & \textbf{Rotazione}
      & $\mathbf{BWT_T}$\\ 
      \hline
      0 & 11 & \$ & \$mississippi & i\\
      1 & 10 & i & i\$mississipp & p\\
      2 & 7 & i & ippi\$mississ & s\\
      3 & 4 & i & issippi\$miss & s\\
      4 & 1 & i & ississippi\$m & m\\
      5 & 0 & m & mississippi\$ & \$\\
      6 & 9 & p & pi\$mississip & p\\
      7 & 8 & p & ppi\$mississi & i\\
      8 & 6 & s & sippi\$missis & s\\
      9 & 3 & s & sissippi\$mis & s\\
      10 & 5 & s & ssippi\$missi & i\\
      11 & 2 & s & ssissippi\$mi & i\\
    \end{tabular}
  \end{table}
  Avendo quindi:
  \[F_T=\mbox{\$iiiimppssss}\mbox{ e }BWT_T=\mbox{ipssm\$pissii}\]
\end{esempio}
L'importanza di questa trasformata è dovuta soprattutto al fatto che sia
\textit{reversibile}, implicando quindi che a partire da $BWT_T$ è possibile
ricostruire $T$. Questo è possibile grazie ad una proprietà intrinseca della
trasformata che viene riassunta nel cosiddetto \textbf{LF-mapping}.
\begin{definizione}
  Dato un testo $T$, tale che $|T|=n$, data la sua $BWT_T$ e il suo array $F_T$
  si definisce \textbf{LF-mapping} come la proprietà per la quale l'$i$-esima
  occorrenza di un carattere $\sigma$ in $BWT_T$ corrisponde all'$i$-esima
  occorrenza dello stesso carattere in $F_T$.
\end{definizione}
Grazie a questa definizione è possibile partire dall'ultimo carattere del testo,
\$, e ricostruire l'intero testo a ritroso. Si vede quindi un breve esempio.
\begin{esempio}
  Si riprende l'esempio precedente, avendo:
  \[BWT_T=\mbox{ipssm\$pissii}\mbox{ e }F_T=\mbox{\$iiiimppssss}\]
  Si comincia dal simbolo \$ in $BWT_T$, che è l'ultimo carattere di $T$. Si
  inoltre ha che esso corrisponde al primo, e unico \$ in $F_T$, all'indice
  $0$. Tale simbolo, per l'ovvia proprietà delle rotazioni è preceduto dal
  simbolo $BWT_T[0]=i$ in $T$. Quindi $i$ precederà \$ in $T$. Si sa inoltre che
  tale $i$ è il primo $i$ in $BWT_T$. Si cerca quindi il primo $i$ in $F_T$,
  sapendo che sono lo stesso simbolo nel testo. A questo punto il simbolo allo
  stesso indice di tale $i$ nella $BWT_T$ sarà il simbolo che precede $i$ nel
  testo. Proseguendo a ritroso si ricostruisce l'intero testo:
  \[T=\mbox{mississippi\$}\]
\end{esempio}
\subsection{Trasformata di Burrows-Wheeler run-length encoded}
Come già introdotto con la \textit{BWT} caratteri uguali tendono ad essere messi
in posizioni consecutive all'interno della trasformata stessa. Si è quindi
pensato, fin da subito, ad un modo efficiente per memorizzare in modo compresso
testi mediante l'uso del \textit{run-length encoding} memorizzando le cosiddette
\textbf{run} di caratteri consecutivi uguali mediante coppie:
\[(\mbox{carattere}, \mbox{lunghezza della run})\]
\begin{esempio}
  Vediamo un breve esempio.\\
  Si ipotizzi di avere la seguente stringa:
  \[s=\mbox{aaaacctgggggg}\]
  Una sua memorizzazione run-length sarebbe:
  \[\{(a,4),(c,2),(t,1),(g,6)\}\]
\end{esempio}
\subsection{R-index}
In questa direzione, nel 2005, M\"{a}niken e Navarro proposero la
\textbf{Run-Length encoded Burrows–Wheeler Transform (\textit{RLBWT})}
\cite{rlbwt}.
Dato un testo $T$, tale che $|T|=n$, la \textbf{RLBWT} di $T$ è la
rappresentazione \textit{run-length encoded} della $BWT_T$ ed è denotata come
$RLBWT_T$. Si ha  che $|RLBWT_T|=r$, dove $r$ è il numero di run.\\
Una strategia per la memorizzazione in modo compatto la \textit{RLBWT}, di $r$
run, è quella di memorizzare:
\begin{itemize}
  \item una stringa $c$, tale che $|c|=r$, contenente un solo carattere per ogni
  run della $BWT_T$, tale che $|BWT_T|=n$
  \item un bitvector $bv$, tale che $|bv|=n$, tale che $bv[i]=1$ sse $BWT_T[i]$
  è il primo carattere di una run
\end{itemize}
\begin{esempio}
  Si prenda ad esempio la seguente $BWT_T$:
  \[BWT_T=acggtcccaa\]
  Si hanno:
  \[c=acgtca\]
  \[bv=1110110010\]
\end{esempio}
Grazie al lavoro di M\"{a}niken e Navarro si giunse al seguente teorema.
\begin{teorema}
  Dato un testo $T$, tale che $|T|=n$, se ne può costruire la \textit{RLBWT} in
  uno spazio $\mathcal{O}(r)$ tale per cui si possono conteggiare tutte le
  occorrenze di un pattern $P$, tale che $|P|=m$, in tempo:
  \[\mathcal{O}(m\log n)\]
\end{teorema}
Nonostante questi ottimi risultati, per poter computare l'\textit{FM-index}, si
richiedeva anche la costruzione dei \textit{suffix array samples} in spazio
$\mathcal{O}(r)$. \\
Grazie al loro indice la struttura era in grado di, dato un testo $T$, tale che
$|T|=n$, e dato un pattern $P$, tale che $|P|=m$: 
\begin{itemize}
  \item conteggiare le occorrenze (\textit{count query}) del pattern nel testo,
  in tempo $\mathcal{O}(m\log n)$, con spazio $\mathcal{O}(r)$  
  \item localizzare tali occorrenze (\textit{locate query}) in tempo
  $\mathcal{O}(s)$, con spazio $\mathcal{O}\left(\frac{r}{s}\right)$, avendo $s$
  come distanza tra due \textit{SA samples}
\end{itemize}
Si ha quindi che i \textit{SA samples} di un ordine di grandezza maggiore, in
termini di memoria, rispetto alla \textit{RLBWT}.\\
A tal proposito, nel 2017, Policriti and Prezza \cite{policriti} mostrarono
come, dato un testo $T$, tale che $|T|=n$, e dato un pattern $P$, tale
che $|P|=m$, trovare l'intervallo nella $BWT_T$ contenente i caratteri in
\textit{occ} che precedono le occorrenze di $P$ in $T$ in spazio
$\mathcal{O}(r)$ e in tempo:
\[\mathcal{O}(m\log\log n)\]
Questo risultato è ora noto in letteratura come \textbf{Toehold Lemma} e
dimostra come identificare \textbf{un} \textit{SA sample} nell'intervallo
contenente una il pattern $P$. Il limite è dato dal fatto che non si supporta la
localizzazione di tutte le $k$ occorrenze degli \textit{SA samples} in
quell'intervallo.\\
Nel 2020 Gagie et al \cite{gagie2020} trovarono soluzione a questo problema,
mediante la definizione della funzione $\varphi$ (che nel dettaglio si
dettaglierà più avanti) che ha permesso di avere le \textit{locate query} in
spazio $\mathcal{O}(r)$. Tale risultato si riassume nel seguente teorema.
\begin{teorema}
  Dato un testo $T$, tale che $|T|=n$, si può memorizzare $T$ in spazio
  $\mathcal{O}(r)$ tale che si possano trovare tutte le $k$ occorrenze di un
  pattern $P$, tale che $|P|=m$, in tempo:
  \[\mathcal{O}((m+k)\log\log n)\]
\end{teorema}
La struttura dati dietro questo risultato è stata denotata \textbf{R-index},
un'evoluzione dell'\textit{FM-index}, consistente in:
\begin{itemize}
  \item la \textit{RLBWT}
  \item i \textit{SA sample}, ovvero i valori di \textit{SA} all'inizio e alla
  fine di ogni run. Si noti quindi che sono memorizzati in spazio proporzionale
  al numero di run
\end{itemize}
Per i dettagli in merito alla costruzione dell'indice si rimanda ai paper di
Kuhnle et al. \cite{kuhnle}, di Mun et al. \cite{mun} e di Boucher et
al. \cite{boucher}. 
\subsection{Match massimali con RLBWT}
Dopo aver introdotto l'\textbf{R-index} bisogna brevemente spiegare come avvenga
effettivamente la ricerca dei match di un pattern $P$, lungo $m$, in un
testo $T$, lungo $M$.\\
Il fine è quindi quello di calcolare i cosiddetti \textbf{Maximal exact matches
  (\textit{MEM})}, ovvero match esatti tra $P$ e $T$ che non possono essere
estesi in alcuna direzione.
\begin{definizione}
  Dato un testo $T$, con $|T|=n$, e un pattern $P$, con $|P|=m$, si definisce
  una sottostringa $P[i,i+l-1]$, di lunghezza $l$, \textbf{MEM} di $P$ in $T$
  se:
  \begin{itemize}
    \item $P[i,i+l-1]$ è una sottostringa di $T$
    \item $P[i-1,i+l-1]$ non è una sottostringa di $T$ (non si può estendere a
    sinistra) 
    \item $P[i,i+l]$ non è una sottostringa di $T$ (non si può estendere a
    destra) 
  \end{itemize}
\end{definizione}
L'importanza nel calcolo dei match massimali esatti si ritrova nel loro uso nei
metodi di allineamento basati sul \textbf{paradigma seed-and-extend}.
Tale paradigma, sfruttato in algoritmi di allineamento come \textbf{BLAST}
\cite{blast}, uno degli allineatori più usati al mondo, si basa sul trovare
\textit{MEM} di piccola lunghezza, i \textit{seed} appunto, per poi continuare
l'allineamento tramite algoritmi più sofisticati, spesso basati sulla
\textit{programmazione dinamica}. \\
Nel 2020, Bannai et al. \cite{bannai} mostrarono come il calcolo dei
\textit{MEM} fosse equivalente al calcolo delle \textbf{Matching Statistics
  (\textit{MS})}, un concetto teorico molto usato in
\textit{bioinformatica}. Informalmente, per ogni posizione $i$ di un pattern
$P$, le \textit{MS} riportano la lunghezza della più lunga sottostringa comune,
iniziante in $i$, tra $P$ e un testo $T$, e l'indice di inizio di tale
sottostringa in $T$.
\begin{definizione}
  Dato un testo $T$, con $|T|=n$, e un pattern $P$, con $|P|=m$, si definisce
  \textbf{matching statistics} di $P$ su $T$ un array $MS$, tale che $|MS|=m$
  di coppie $(pos, len)$ tale che:
  \begin{itemize}
    \item $T[MS[i].pos,MS[i].pos+MS[i].len-1]=P[i,i+MS[i].len-1]$, quindi si ha
    un match tra $P$ e $T$ lungo $MS[i].len$ a partire da $MS[i].pos$ in $T$ e
    da $i$ in $P$
    \item $P[i,i+MS[i].len]$ non occorre in $T$, quindi il match non è
    ulteriormente estendibile 
  \end{itemize}
\end{definizione}
Una volta calcolato \textit{MS} si ha il seguente lemma.
\begin{lemma}
  Dato un testo $T$, con $|T|=n$, un pattern $P$, con $|P|=m$, e il
  corrispondente array di matching statistics $MS$ si ha che:
  \[P[i,i+l-1],\forall 1<i\leq m\]
  è un \textbf{MEM} di lunghezza $l$ in $T$ sse:
  \[MS[i].len=l\land MS[i-1].len\leq MS[i].len\]
\end{lemma}
\textbf{CAPIRE SE METTERE DETTAGLI DI CALCOLO CON LCP}.\\
Per costruire l'array \textit{MS} l'approccio naive è quello di sfruttare
interamente l'\textit{LCP array} ma, sempre nell'articolo di Bannai et
al.\cite{bannai}, si è presentato una semplice concetto in grado di
ottimizzare il processo, quello delle \textbf{threshold}. Questa piccola
struttura dati memorizza il minimo valore dell''\textit{LCP array} tra due run
consecutive nel medesimo simbolo nella \textit{BWT}.
\begin{definizione}
  Dato un testo $T$ e date $BWT_T[j',j]$ e $BWT_T[k,k']$ due run consecutive
  dello stesso carattere in $BWT_T$. Si definisce la \textbf{threshold}
  posizione:
  \[j< i \leq k\mbox{ tale che } i\mbox{ è l'indice del minimo valore in
    }LCP[j+1,k],\] 
\end{definizione}
Rossi et al., nel 2021, sfruttarono tutte le conoscenze relative
alla \textbf{RLBWT}, all'\textbf{R-index} e alle \textbf{matching statistics}
per ideare \textbf{MONI:\textit{ A Pangenomics Index for Finding MEMs}}
\cite{moni}. In questa soluzione si ha quindi la costruzione, in due
\textit{sweep}, tramite l'\textbf{algoritmo di Bannai}, dell'array delle
\textit{matching statistics}. Infatti si ha:
\begin{itemize}
  \item un primo sweep che computa i vari $MS[i].pos$
  \item un secondo sweep che, tramite random access sul testo $T$, computa i
  vari $MS[i].len$ e, contemporaneamente, annota i match
\end{itemize}
\textbf{APPROFONDIRE ULTIMA FRASE}\\
\textbf{CAPIRE SE METTERE ESEMPIO}\\
Questa pubblicazione è stata uno dei punti di partenza per
riadattare quanto studiato sulla \textit{BWT} classica al fine di ottenere
risultati analoghi per la \textit{PBWT}.\\
Per ulteriori dettagli sull'implementazione, sul calcolo delle
\textit{threshold} e sui risultati si rimanda direttamente al paper di
\textit{MONI} \cite{moni}.
\subsection{PHONI}
Nel 2021, Boucher, Gagie, Rossi et al. proposero un ulteriore miglioramento di
quanto fatto in \textit{MONI}, con \textbf{PHONI: \textit{Streamed Matching
    Statistics with Multi-Genome References}}.\\
In questo progetto non solo si sostituì l'uso delle \textit{thresholds} con
l'uso delle \textbf{LCE queries}, riducendosi ad un solo \textit{sweep}
sull'array $MS$ (permettendo un uso ``online'' dell'algoritmo), ma si
esplicitò anche l'uso delle \textit{funzioni} $\mathbf{\varphi}$ e
$\mathbf{\varphi^{-1}}$ e dell'$PLCP_T$ per il riconoscimento di tutte le
occorrenze di ogni \textit{MEM} tra un pattern e un testo.\\
A tal fine si sfrutta infatti il seguente teorema \cite{gagie2020}.
\begin{teorema}
  Dato un testo $T$, tale che $|t|=n$, si può memorizzare $T$ in
  $\mathcal{O}(r)$, con $r$ numero di run, tale che, dato un indice
  $p\in\{0,n-1\}$ si possano computare $\varphi(p)$, $\varphi^{-1}(p)$ e
  $PLCP[p]$ in tempo:
  \[\mathcal{O}(\log\log n)\]
\end{teorema}
Si è quindi potuto migliorare e semplificare l'\textbf{algoritmo di Bannai}
usato in \textit{MONI} sfruttando un solo \textit{sweep}. Infatti, sfruttando le
\textit{LCE query}, avendo il testo $T$ in memoria sotto forma di \textit{SLP},
è possibile computare contemporaneamente sia i vari
$MS[i].pos$ che i vari $MS[i].len$, avendo, come nel caso dell'\textit{algoritmo
  di Bannai}, anche il computo dei match nel momento in cui si hanno a
disposizione i valori $MS[i].len$, avendo che si ha un \textit{MEM} sse
$MS[i].len=l\land MS[i-1].len\leq MS[i].len$.\\ 
Per ulteriori approfondimenti si rimanda al paper di \textit{PHONI}
\cite{phoni}.
\textbf{CAPIRE SE SERVE APPROFONDIMENTO SU COMPLESSITÀ CHE PERÒ NON SONO BEN
  DEFINITI IN QUANTO DIPENDENTI DALLA STRUTTURA SOTTOSTANTE}