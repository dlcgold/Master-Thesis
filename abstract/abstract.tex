\documentclass[a4paper,11pt, oneside]{article}
\usepackage[left=15mm, right=15mm, top=15mm, bottom=20mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{engrec}
\usepackage{rotating}
\usepackage{verbatim}
\usepackage[safe,extra]{tipa}
% \usepackage{showkeys}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{fontspec}
\usepackage{enumerate}
\usepackage{physics}
\usepackage{braket}
\usepackage{mhchem}
\usepackage{marginnote}
\usepackage{pgfplots}
\usepackage{cancel}
\usepackage{polynom}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{pdfpages}
\usepackage{pgfplots}
\usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage[cache=false]{minted}
\usepackage{mathtools}
\usepackage[noend]{algpseudocode}

\newcommand*{\bfrac}[2]{\genfrac{}{}{0pt}{}{#1}{#2}}

\usepackage{tikz}\usetikzlibrary{er}\tikzset{multi  attribute /.style={attribute
    ,double  distance =1.5pt}}\tikzset{derived  attribute /.style={attribute
    ,dashed}}\tikzset{total /.style={double  distance =1.5pt}}\tikzset{every
  entity /.style={draw=orange , fill=orange!20}}\tikzset{every  attribute
  /.style={draw=MediumPurple1, fill=MediumPurple1!20}}\tikzset{every
  relationship /.style={draw=Chartreuse2,
    fill=Chartreuse2!20}}\newcommand{\key}[1]{\underline{#1}}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{arrows,shapes,backgrounds,petri}
\tikzset{
  place/.style={
    circle,
    thick,
    draw=black,
    minimum size=6mm,
  },
  transition/.style={
    rectangle,
    thick,
    fill=black,
    minimum width=8mm,
    inner ysep=2pt
  },
  transitionv/.style={
    rectangle,
    thick,
    fill=black,
    minimum height=8mm,
    inner xsep=2pt
  }
} 
\usetikzlibrary{automata,positioning,chains,fit,shapes}
\usetikzlibrary{circuits.logic.US}
\usetikzlibrary{positioning}
\usepackage{fancyhdr}
% \pagestyle{fancy}
% \fancyhead[LE,RO]{\slshape \rightmark}
% \fancyhead[LO,RE]{\slshape \leftmark}
\fancyfoot[C]{\thepage}
\usepackage[usenames,dvipsnames]{pstricks}
\usepackage{epsfig}
\usepackage{pst-grad} % For gradients
\usepackage{pst-plot} % For axes
\usepackage[space]{grffile} % For spaces in paths
\usepackage{etoolbox} % For spaces in paths
\makeatletter % For spaces in paths
\patchcmd\Gread@eps{\@inputcheck#1 }{\@inputcheck"#1"\relax}{}{}
\makeatother
\usepackage{lipsum}
\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}
\title{PhD Proposal}
\author{Davide Cozzi, 829827,
  \href{mailto:d.cozzi@campus.unimib.it}{d.cozzi@campus.unimib.it}} 
\date{}
\makeatletter
\renewcommand{\paragraph}{%
  \@startsection{paragraph}{4}%
  {\z@}{1.25ex \@plus 1ex \@minus .2ex}{-1em}%
  {\normalfont\normalsize\bfseries}%
}
\makeatother

\pgfplotsset{compat=1.13}
\begin{document}
\maketitle

\definecolor{shadecolor}{gray}{0.80}
\setlist{leftmargin = 2cm}
\newtheorem{teorema}{Teorema}
\newtheorem{definizione}{Definizione}
\newtheorem{esempio}{Esempio}
\newtheorem{corollario}{Corollario}
\newtheorem{lemma}{Lemma}
\newtheorem{osservazione}{Osservazione}
\newtheorem{nota}{Nota}
\newtheorem{esercizio}{Esercizio}
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\MYhref}[3][blue]{\href{#2}{\color{#1}{#3}}}%
\subsubsection*{Introduction}
The problem of \textit{pattern matching} is one of the most studied topics in
the field of algorithmics and bioinformatics. The interest in such
problems is due to the need to align sequences or to search for specific
patterns within the \textit{DNA}. 
In this context, a large number of data structures and algorithms have been
modeled. Among these, one of the most used is the \textbf{Burrows-Wheeler
  transform (\textit{BWT})}, thanks to the studies of Ferragina and Manzini, who
proposed its use together with indexing via \textit{FM-index}.\\
Furthermore, in recent years, there has been a change of interest in the field
of bioinformatics. Until a few years ago the research was focused on the
study of a \textbf{linear sequence of a genome} while now the 
researchers are beginning to deepen the topic of the \textbf{pangenome}, which
term was 
introduced by Tettelin in 2005. In fact, the need to take into 
account the high variability in population genomes as well as the specificity of
an individual genome in a personalized approach to medicine is rapidly pushing
the abandonment of the traditional paradigm of using a single reference genome
\cite{pangenome}. \\
Thanks to the last developments in sequencing technologies, which had led both
to reduce the costs of single sequencing and to produce sequences of ever higher
quality in less and less time, the researchers were able to theorize the
\textbf{pangenome graph}. Furthermore, the new amount of sequences has led to
new algorithms regarding the problem of \textit{pattern matching}. In 2021,
Rossi et al. proposed \textit{MONI} as a data structure to handle a 
\textbf{run-length encoded version of BWT (\textit{RLBWT})}, with the ultimate
intention of indexing and using multiple genomes as a reference
\cite{moni}. Together with this data structure, the authors proposed the concept
of \textbf{matching statistics (\textit{MS})} in order to efficiently compute
the matches between a pattern and a text. A recent improvement has been made
through the implementation of \textit{PHONI} \cite{phoni}, where the
\textbf{longest-common-extension (\textit{LCE}) queries} in order to further
optimize the pattern search.\\ 
From a biological point of view, it is interesting to note that, as it has been
pointed out by the study of \textit{1000 Genome Project}, there are over 88
million variants between those human genomes. Among these variants 84.7 million
are \textbf{Single 
  Nucleotide Polymorphisms (\textit{SNPs})}, 3.6 million are \textbf{short
  insertions/deletions (\textit{indel})} and 60000 are \textbf{structural
  variants}, 
involving more than 
50 nucleotides. All of them are now a limit to the traditional use of a
\textit{linear sequence of a genome}.\\
Against this background, various algorithms and data structures have
been implemented in order to study \textit{haplotypes} and
\textit{genotypes}, such as the \textbf{genotyping variants
  problem}. Briefly, we could define \textit{haplotypes} as a combination 
of allelic variants, each one inherited from a parent. Instead, the
\textit{genotype} is the complete set of genes contained in the DNA. So, from
2005, publication of the \textbf{genome-wide association studies
  (\textit{GWAS})} has begun. The goal of this type of studies is to screen the 
\textit{pangenome} looking for associations between genetic variants, in order,
for example, to study also deseases outocomes.
In this particular historical period it is also impossible not to mention the
viruses. In fact, by nature, during infections, viruses replicate a lot but
often in a not perfect way, thus producing many inexact clones, referred as
\textbf{viral haplotypes} which, taken together, form the \textbf{viral
  pangenome}. The identification of all this haplotypes is crucial both for the
effective study of the spread of the virus and for the production of effective
drugs, in a content of high pharmacological resistance. \\
One of the most important data structure, developed in order to handle the study
of haplotypes sequences, is the \textbf{positional Burrows-Wheeler transform
  (\textit{PBWT})}, proposed by Durbin in 2014 \cite{pbwt_durbin}. Using this
particular data structure (which will be described below), it is possible to
study efficiently a collection of haplotypes but only in the bi-allelic
case. Furthermore variants of the \textit{PBWT} have been studied for
handling the multiallelic case. The use of the \textit{PBWT} is found in many
software for the study of haplotypes and in various genotype imputation methods,
that are studies that infers unobserved genotypes in a sample of individuals
\cite{impute5}.\\
During the development of my master thesis, I worked to create a run length
encoded variant of the \textit{PBWT}, the \textbf{RLPBWT}, using and adapting
the various theories developed for the \textit{RLBWT}, in collaboration with the
authors of \textit{MONI} and \textit{PHONI}. In this context, my PhD
is going to be focused on the development of new algorithms in
various topics related to open problems in the study of the
\textit{haplotyping/genotyping}, of the \textit{genome 
  variants} and of the imputation issues related. My intention is also to deepen
experimental themes  
of \textit{pattern matching} and of the \textit{pangenome graph}, in detail the
new developments on \textit{BWT} and indexing structures, as well as
\textit{succinct data structures}, with particular attention to the use of
\textit{bitvectors}.
\subsubsection*{State of the art}
Now I present a brief overview of the main algorithms, data structures,
methods etc $\ldots$ that will be the core of my studies during PhD. 
% \paragraph*{BWT}
% The \textbf{Burrows-Wheeler Transform (\textit{BWT})} was introduced
% in 1994 in order to compress texts but it has been used widely
% in bioinformatics, above all thanks to the already cited
% \textit{FM-index}. Given a text $T$, \$-terminated, such that $|T|=n$, we can
% define, denoting with $SA_T$ the \textit{suffix array} of $T$, the $BWT_T$ as:
% $BWT_T[i] = T[SA_T[i]-1]$, if $SA_T[i]>0$, and $BWT_T[i] = \$ $ otherwise. Less
% formally, we can say that $BWT_T[i]$ is the character that precedes the $i$-th
% suffix in the lexicographical order. It is important to note that this
% transform is reversible, so we can reconstruct the text $T$ from its transform
% $BWT_T$ using the \textbf{LF-mapping}. Given $BWT_T$ and an array,
% called $F_T$, with all the characters of $T$ in the lexicographical order, we
% can say that, thanks to the \textit{LF-mapping}, the $j$-th occurrence of a
% certain character in $BWT_T$ corresponds to the $j$-th occurrence of the same
% character in $F_T$. Thank to this we can reconstruct $T$ starting from its last
% character 
% \$. With the use of the \textit{LF-mapping}, we can perform the
% \textit{backward-search} in order to use the $BWT_T$ to look for a pattern $P$
% within $T$. This can be done efficiently thank to the \textit{FM-index} which
% consists of two functions. The first one is $C$ function, such that, given an
% alphabet $\Sigma$ (that includes the ending character \$),
% $C:\Sigma\to[1,n]$. This function, given a character $\sigma\in \Sigma$, returns
% the number of occurrences of characters lexicographically minor in $T$ than the
% one given as argument. The second one is the $Occ$ function,
% $Occ:\Sigma\times[1,n]\to[1,n]$, which has a character $\sigma\in\Sigma$ 
% and an index $i$ of $BWT_T$ as arguments and returns the count of occurrences of
% $\sigma$ in $BWT_T[1,i]$.\\
% The use of \textit{BWT} has allowed the construction of efficient algorithms
% both in the field of pattern matching and sequence alignment. 
\paragraph*{Bitvectors}
\textbf{Bitvectors} are one of the most important data structure when
mentioning \textit{succinct data structures}. \\
A \textit{bitvector} is an array on $n$ bits which allows two particular
operations, called \textbf{rank} and \textbf{select}, in addition to the classic
operations on boolean arrays, such as \textit{random access} in
\textit{constant time}. More in detail, the \textit{rank
function} allows to calculate how many occurrences of one are up to a certain
index. Instead, the \textit{select function} allows to obtain the index of every
one present in the \textit{bitvector}. Formally, given a bitvector $B$, such
that $|B|=n$, and given an index $i$, such that $0\leq i<n$, we can define
$rank_B(i)=\sum_{k=0}^{k<i} B[k]$. Instead, about the select function, given an
integer $i$, such that $0<i\leq rank_B(n)$, where $n=|B|$, we can define
$select(i)=\min\{j \,| \,\, rank_B(j+1)=i\}$.\\ 
From a theoretical point of view these two operations can be supported in
\textit{constant time}, with the additional cost of $\mathcal{O}(n)$ bits in
memory. In more practical terms, there are several implementations of the 
same within \textbf{SDSL (\textit{Succint Data Structures Library})}, one of the
most important C++ library used in bioinformatics. As 
the implementation changes (for example \textit{plain bitvector, interleaved
  bitvector, sparse bivector} etc$\ldots$) the computational time of the two
operations varies (usually only one of the two is in constant time) as well as
the amount of additional bits needed. An example of the use of bitvectors is
tracking the runs in the run-length encoded implementations of \textit{BWT} and
\textit{PBWT}, where we put one at each head of run, allowing fast operations
along the runs themselves. 
\paragraph*{RLBWT}
The \textbf{Burrows-Wheeler Transform (\textit{BWT})} was introduced
in 1994 in order to compress texts but it has been used widely
in bioinformatics, above all thanks to the already cited
\textit{FM-index}.\\
Speaking of \textit{pangenome}, linear indexing via FM-index is no longer the
best solution as it does not handle the large repetitions there are in this new
type of sequences. In 2005 M\"{a}niken and Navarro defined 
the \textbf{Run-Length Burrows–Wheeler Transform (\textit{RLBWT})}. Given a text
$T$, $RLBWT_T$ is a rappresentation of $BWT_T$ with a compact storage of
consecutive equal characters, the so-called \textit{runs}. With this new
perspective, the algorithms have changed from being linear over the length of
the text, $n$, to being linear over the number of runs, $r$, so sub-linear over
the length of the text. \\ 
The new indexing method, introduced by Gagie et al., was called \textbf{r-index}
and it corresponds to the \textit{RLBWT} plus the \textit{suffix array sampling}
at the beginning and at the end of every run. The algorithm for querying through
the \textit{RLBWT} takes advantage of other methods, such as the use of
\textbf{thresholds} (minimum \textit{LCP} value between two consecutive runs of
the same character) in \textit{MONI}, and the use of \textbf{longest common
  extension (\textit{LCE}) query} (to compute the right equal common extension
between two position in the text) in \textit{PHONI}. Both solutions use
\textbf{straight-line programs (\textit{SLP})}, for \textit{random 
  access} in \textit{MONI} and for \textit{Longest Common extensions (LCE)
  queries} in \textit{PHONI}. 
The purpose of the two projects is computation of the \textbf{matching
  statistics (\textit{MS})}. Given a pattern $P$ and a text $T$, the \textit{MS}
of $P$ in respect to $T$ is an array $M$ of pairs position/length ($pos$/$len$),
$|M|=|P|$, such that $T[M[i].pos:M[i].pos+M[i].len-1]=P[i:i+M[i].len]-1]$ and
$P[i:i+M[i].len]$ does not occur in $T$. Given \textit{MS}, we can compute every
\textbf{Maximal Exact Match (\textit{MEM})} of a pattern in a text. Given a
text $T$ and a pattern $P$, a substring of the pattern $P[i : i+l-1]$, of length
$l$, is a \textit{MEM} of $P$ in $T$ if $P[i:i+l-1]$ is a substring of $T$ but
neither $P[i-1:i+l-1]$ nor $P[i:i+l]$ are, so if the substring cannot be
extended either right or left. Furthermore, using a particular
function called $\varphi$ (and and its inverse $\varphi^{-1}$), based on the use
of the \textbf{inverse suffix array (\textit{ISA})}, it has been possible to
find all starting positions of all copies of $P$ in $T$ from starting position
of the match extracted by \textit{MS}, quickly calculating, given a position $p$
in $SA$, the previous and next position in the suffix array. \\
% More formally, given a starting position
% $p$, $\varphi(p)=SA[ISA[p]-1]$, $NULL$ if $ISA[p]=0$, and
% $\varphi^{-1}(p)=SA[ISA[p]+1]$, $NULL$ if $ISA[p]=|T|-1$, where $ISA[i]=j$ iff
% $SA[j]=i$.\\
Thanks to these and other methods, it was possible to perform pattern matching
efficiently even on long sequences of nucleotides, such as those studied in
a pangenomic context. In fact, most studies in the field of bioinformatics start
with the resolution of pattern matching problems and, as a direct consequence,
of alignment problems. 
\paragraph*{PBWT}
Based on the theories of the \textit{BWT}, in 2014, Durbin devised the
\textbf{positional Burrows–Wheeler transform (\textit{PBWT})}, in order to
solve the problem of pattern matching on panels (matrices) of haplotypes. In
detail, he analyzed a panel $X$ with $M$ haplotypes and $N$ biallelic
sites. This data structure is based on a \textit{reversed-prefix ordering} at
each column $k$ that produces two different multidimensional arrays. The first
one is the set of the \textbf{prefix arrays}, denoted by $a$, which contains the
index of the haplotype $m$ in the original panel, for each column $k$ and for  
each position $i$ of $a_k$. More formally, we can say that $a_k[i]=m$ iff $X_m$
is the $i$-th haplotype in the reversed-prefix ordering at column $k$. We can
note $x_m$, such that $a_k[i]=m$, could be denoted by $y_i^k$. The second
bidimensional array is the set of the \textbf{divergence arrays}, denoted by
$d$, which indicates the index of the starting column of the longest common
suffix, ending in column $k$, between a row and its previous one, at
reversed-prefix ordering at column $k$. More formally, we can define $d_k[i]=h$
iff $h$ is the smallest column index such that $y_i^k[h,k)=y_{i-1}^k[h,k)$. \\
Thanks to these two bidimensional arrays, it is possible to compute all matches
within $X$ longer than a minimum length $L$, all set-maximal matches within $X$
in linear time and all set-maximal matches (which we could also call ``MEMs'')
from a new sequence $z$ to $X$ in $\mathcal{O}(M^2N)$.
Despite the fact that \textit{PBWT} has been poorly regarded in the scientific
community in the early years of its development, there has been a growth in
research based on it, both in terms of variant design, such as the already
mentioned multiallelic version or the \textbf{dynamic PBWT (\textit{d-PBWT}}),
and in terms of its use to study haplotype panels.\\
A first example could be found in the paper of Alanko et al., published in
2021, where the authors used the \textit{PBWT} to look for \textit{maximal
  perfect haplotype block}, within a haplotypes panel. These types of
algorithms are essential for the identification of genomic regions that show
signatures of natural selection.\\
Another interesting paper is the one by Wlliams and Mumey, published in 2020,
who also studied \textit{maximal perfect haplotype blocks}, but with the
addition of \textit{missing data}, handled with the help of wildcards, using the
\textit{PBWT}. \\ 
A very intriguing tool, talking about \textit{GWAS}, to be cited is
\textbf{IMPUTE5}, proposed by Rubinacci et al. in 2020. The main purpose of this
software is to make genotype imputation in order to predict unobserved genotypes
from a panel with milions of haplotypes. Due to the size of the panel, the use
of \textit{PBWT} proved inevitable, further demonstrating the importance of this
data structure.\\
Durbin himself, the author of the \textit{PBWT}, with Shchur et al. in 2019,
proposed a use of his data structure in \textit{GWAS} context. Infact they
studied the associations between genetic variants in order to identify signals
of natural selection to build the so-called \textbf{ancestral recombination
  graph}, that contains complete informations about samples history.\\
For the sake of completeness, an example where \textit{PBWT} is not used is
\textbf{Ranbow}, proposed by Moeinzadeh et al. in 2020. The 
aim of this project was the haplotype reconstruction of polyploid genome from
short read sequencing data, studying also the multi-allelic case.
\subsubsection*{Research goals}
Therefore, for my master's thesis, I had to rethink the concept of
\textit{Matching Statistics} for \textit{PBWT} (tracking  a \textit{row} of the
panel instead of the \textit{pos}), how to compute the \textit{SLP}
for the panel, how to use \textit{thresholds}/\textit{LCE queries} and how to
get the same behaviour of the $\varphi$ function, in order to obtain the
\textbf{RLPBWT}, combining the ideas related to the \textit{RLBWT} with those of
the \textit{PBWT}, eventually, as for example for the $\varphi$ function,
developing a simple new data structure.\\  
% More formally, given a panel $X$ (where every row is 
% defined by $X_j$) and a pattern $P$, $|P|=X_{width}=n$, we define \textit{MS} as
% an array of length $n$, such that, for each position $1\leq i\leq n$,
% $X_{MS[i].row}[i-MS[i].len+1: i]=P[i-MS[i].len+1:i]$ and $P[i-MS[i].len:i]$ is
% not a suffix of $X_1[1:i],\ldots,X_{X_{height}}[1:i]$.
% Thresholds are defined
% by the index of the minimum \textit{LCP value} inside a run, considering also
% the \textit{LCP value} of the head of the next run, if it exists. We stretch the
% reverse panel (from the right to the left) to get the \textit{SLP}, in order to
% make \textit{LCE queries} possible. This has to be done because \textit{LCE
%   queries} are computed, between two rows, from the right to the left. Instead,
% regarding the $\varphi$  function and its inverse, I have implemented a new
% simple data structure to identify which prefix array values are above and below
% a given index in a column of the \textit{RLPBWT matrix}, that is the panel
% permuted via the prefix array and run-length encoded.
Obviously, there are some limitations, as in Durbin's \textit{PBWT}, such as
studying only biallelic panels and ignoring the management of missing data,
which are very frequent in real cases. \\
Especially during my master’s degree I have deepened both the most theoretical
issues related to algorithms, especially in the field of bioinformatics, that
the most modeling and inference issues, related to the field of computational
systems biology, so it is my interest to continue my studies with the PhD
in order to be able to deepen the computer science potential in the biological
context. Summarizing, some of the research goals of my \textit{Phd} are:
\begin{enumerate}[leftmargin=.2in]
  \setlength\itemsep{-0.2em}
  \item \textbf{Multiallelic data}. Thanks to the increasing growth in the
  production of genotype data the number of multiallelic sites is expected to
  grow, as the number of samples increases, even in the human
  genome, although now, for example, there is only a 2\% presence of triallelic
  sites. Moreover, not only could such sites be more than expected but they are
  usually not considered by most tools. A first step in this direction was made
  by Naseri with the already cited \textit{m-PBWT}. From the point of view of
  spatial complexity, especially about stored arrays for the \textit{FM-index},
  a run-length encoded version of the same could 
  allow the management of increasingly large data for the imputation phases. 
  In this context the aim is to implement a new 
  version of the \textit{RLPBWT} that can handle multiallelic data, adapting the
  current use of bitvectors to handle efficiently the \textit{LF-mapping} also
  with more than two alleles. 
  \item \textbf{Missing data}. A second extension, that would be more
  complicated to 
  handle, is a \textit{RLPBWT} version that admits the presence of
  missing data. Most algorithms and data structures mentioned above assume that
  they work on exact data. In reality, real data can contain errors or even
  gaps, both mainly due to sequencing machines that are not yet perfect,
  although they now have very high correctness rates. Unfortunately, although 
  most of them are corrected as a preliminary step (mostly by heuristic
  methods), this is still an open problem. The development of a data structure
  based on the \textit{PBWT}, and as a next step on the \textit{RLPBWT}, that
  can manage missing data would allow the 
  creation of more and more complete tools for the study of haplotypes and for
  the various \textit{GWAS}. 
  Handling missing data is known to be \textit{hard} so I should
  probably deal with parametric algorithms or approximate algorithms, based on
  researches already developed in \textit{BIAS}.
  \item \textbf{Genotype imputation}. As introduced, having developed a
  wide-range interest in computer science potential in biological studies, I do
  not intend to forget the actual use of the data structures that I will study
  and develop during my PhD. Including the management of missing data and
  multiallelic data can further improve the state of the art of genotype
  imputation, allowing even more precise inference of unobserved genotypes, and,
  in more detail, of \textit{GWAS}. All this must be read in the perspective of
  a continuous increase of the available data, moreover not not only regarding
  the human sequencing.  
\end{enumerate}
From a more technical point of view I mainly focused on the use of the
\textbf{C++ programming language}, during both my bachelor and master
thesis projects. I managed to do this because of the availability of efficient
libraries, as the already 
cited \textit{SDSL}, that has become a standard in bioinformatics. However, in
addition to \textit{C++}, I had the opportunity to deepen 
\textbf{Python}, with libraries such as \textit{biopython}, and \textbf{Rust},
with recently developed libraries such as \textit{bio-rust} (even if still not
complete from an algorithmic point of view). Another point of interest is the
analysis and the development of efficient algorithms based on parallel computing
(also on GPU), that are increasingly in use in both \textit{bioinformatics} and
\textit{computational systems biology}. \\  
To conclude this proposal, I also would point out the intention to remain in
contact with various researchers, with whom I have already collaborated during 
my master thesis project, including Christina Boucher (University of Florida)
and Travis Gagie (Dalhousie University), in order to improve the quality of my
PhD program.  
\bibliographystyle{unsrt}
\bibliography{abstract}
% \newpage
% \noindent
% \begin{shaded}
%   \noindent
%   \textbf{Note}:
%   \begin{itemize}
%     \item il font dovrebbe essere corretto secondo le richieste. Non ho visto
%     specifiche relative ai margini del file
%     \item nell'introduzione mancano alcuni concetti relativi allo
%     studio diretto del grafo pangenomico. La professoressa Bonizzoni aveva
%     citato ad esempio le \textit{superbubbles}. Una volta chiariti gli scopri
%     precisi 
%     del progetto di ricerca dovrò sistemare (oltre che aggiungere eventuali
%     aspetti non considerati e toglierne altri)
%     \item nell'introduzione devo capire come ordinare meglio i concetti
%     \item nell'introduzione forse servono esempi di casi d'uso relativi alle
%     varie problematiche. Probabilmente è necessaria una parte più discorsiva
%     relativa anche all'importanza biologica di questo genere di studi
%     \item la conclusione dell'introduzione, dove si parla di cosa verrà
%     eventualmente approfondito durante il PhD, è praticamente un
%     \textit{placeholder}  
%     \item l'incipit allo stato dell'arte è sicuramente da modificare
%     \item la struttura dell'intera sezione riguardante lo stato dell'arte penso
%     debba essere rivista in base a standard per la stesura della proposal che
%     non conosco
%     \item nello stato dell'arte la parte relativa alla BWT è commentata in
%     quanto, pur avendola scritta, non la ritengo essenziale (soprattutto avendo
%     a che fare con un limite di 4 pagine)
%     \item nello stato dell'arte la sezione relativa ad alcuni esempi di studio
%     sugli aplotipi deve essere profondamente rivista in luce soprattutto dei
%     research goals. Un esempio potrebbe essere la citazione, ad esempio, di
%     HapCol, qualora i goals si spostassero un po' dalla PBWT e dai suoi usi più
%     immediati 
%     \item per ignoranza mia su come vada scritto questo tipo di documento non
%     sono sicuro che alcuni dettagli formali nei vari passaggi dello stato
%     dell'arte siano sensati da mettere. Stesso discorso vale nell'incipit dei
%     research goals
%     \item la citazione di quanto svolto per la tesi magistrale non so se sia
%     necessario, e, qualora lo fosse, se la collocazione all'inizio dei research
%     goals sia corretta, ne tantomeno se sia troppo riassunto/esteso
%     \item mancano le specifiche dei fini della ricerca, da chiarire coi docenti
%     (e da cui dipendono diversi punti della proposal)
%     \item la conclusione del research goals non so se sia necessaria
%     \item per quanto riguarda la scelta delle reference nel file \texttt{.bib}
%     ne sono 
%     presenti diverse. In ogni caso, per la prima selezione fatta, ritengo le
%     prime due necessarie mentre la terza e la quarta sono per diversi punti di
%     vista superflue. Avendo un bound di cinque citazioni c'è comunque spazio di
%     manovra
%   \end{itemize}
% \end{shaded}
% \begin{shaded}
%   \textbf{TODO}:
%   \begin{itemize}
%     \item in introduzione parlare di GWAS
%     \item in introduzione parlare della ricerca degli ancestor
%     \item in stato dell'arte citare IMPUTE5
%     \item nei fini della ricerca spingere su importanza dell'imputation e del
%     caso multiallelico
%   \end{itemize}
% \end{shaded}
\end{document}